# Research Paper Implementations

Welcome to the repository of implementations for various influential research papers in the field of Natural Language Processing, Computer Vision, Deep Learning and Artificial Intelligence. Each implementation is designed to be as close to the original paper as possible while being easy to understand and modify.

## Implemented Papers

| Paper Name | ArXiv Link |
|------------|------------|
| [Attention Is All You Need](./Attention%20is%20all%20you%20need) | [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) |
| [RoFormer: Enhanced Transformer with Rotary Position Embedding](./RoPE) | [arXiv:2104.09864](https://arxiv.org/pdf/2104.09864) |
| [Word2Vec Efficient Estimation of Word Representations in Vector Space](./Word2Vec) | [arXiv:1301.3781](https://arxiv.org/pdf/1301.3781) |
| [U-Net: Convolutional Networks for Biomedical Image Segmentation](./UNET%20Semantic%20Segmentation) | [arXiv:1505.04597](https://arxiv.org/abs/1505.04597) |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](./bert) | [arXiv:1810.04805](https://arxiv.org/abs/1810.04805) |
| [Auto-Encoding Variational Bayes (VAE)](./VAE) | [arXiv:1312.6114](https://arxiv.org/abs/1312.6114) |
| [Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)](./pixpix) | [arXiv:1611.07004](https://arxiv.org/abs/1611.07004) |
| [Unsupervised Representation Learning with Deep Convolutional GANs (DCGAN)](./dcgan) | [arXiv:1511.06434](https://arxiv.org/abs/1511.06434) |


## Setup

To set up the repository, please follow the instructions below:

```bash
git clone https://github.com/henilp105/Research-paper-implementations.git
cd Research-paper-implementations
```

## Usage

Each paper's implementation is contained within its own directory. To run an implementation, navigate to the respective directory and follow the instructions in the `README.md` file within that directory. 

Example:
```bash
cd 'Attention is all you need'
python main.py
```

Feel free to reach out if you have any questions or suggestions. Happy coding!



### GPT2, resnet/lenet, cross attention, 
flash attention, sparse attention, MQA, MOE, 
gemma, bpe tokeniser, llama2 , stable diffusion, 
GQA, bert, VAE, ALBERT, micrograd.

clean:
gan, AE.
