{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7knF8LxcjF6_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Data Loading and Tokenization ---\n",
    "# Load WMT14 Czech-English dataset\n",
    "\n",
    "datasets = load_dataset(\"wmt/wmt14\", \"cs-en\")\n",
    "\n",
    "# Use a pretrained tokenizer for inputs and targets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 256\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(batch):\n",
    "    # Extract source and target sentences\n",
    "    sources = [ex['cs'] for ex in batch['translation']]\n",
    "    targets = [ex['en'] for ex in batch['translation']]\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sources, padding='max_length', truncation=True, max_length=max_length)\n",
    "    labels = tokenizer(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing and set format\n",
    "tokenized = datasets.map(preprocess, batched=True, remove_columns=datasets['train'].column_names)\n",
    "tokenized.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "# DataLoader for training\n",
    "dataloader = torch.utils.data.DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5b3IUEuFw9z6"
   },
   "outputs": [],
   "source": [
    "# --- Model Components ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.scaling = d_k ** -0.5\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: (batch, heads, seq_len, d_k)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(self, q,kv, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        # Linear project and split into heads\n",
    "        q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(kv).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(kv).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        out = self.attention(q, k, v, mask)  # (batch, heads, seq_len, d_k)\n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention + residual + norm\n",
    "        attn_out = self.dropout(self.self_attn(x,x, mask))\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # FFN + residual + norm\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        return self.norm2(x + ffn_out)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, look_ahead_mask=None, padding_mask=None):\n",
    "        # Masked self-attention\n",
    "        attn1 = self.dropout(self.self_attn(x,x, look_ahead_mask))\n",
    "        x = self.norm1(x + attn1)\n",
    "        # Encoder-decoder attention\n",
    "        attn2 = self.dropout(self.cross_attn(x, enc_out, padding_mask))\n",
    "        x = self.norm2(x + attn2)\n",
    "        # FFN\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        return self.norm3(x + ffn_out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model,num_heads,d_ff) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model,num_heads,d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_padding_mask(self, seq):\n",
    "        # seq: (batch, seq_len)\n",
    "        return (seq != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def generate_look_ahead_mask(self, size):\n",
    "        mask = torch.tril(torch.ones((size, size),device=\"cuda\")).bool()\n",
    "        return mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src, tgt: (batch, seq_len)\n",
    "        src_mask = self.generate_padding_mask(src)\n",
    "        tgt_mask = self.generate_padding_mask(tgt) & self.generate_look_ahead_mask(tgt.size(1))\n",
    "\n",
    "        # Embedding + Positional Encoding\n",
    "        enc = self.pos_emb(self.token_emb(src) * math.sqrt(self.token_emb.embedding_dim))\n",
    "        for layer in self.enc_layers:\n",
    "            enc = layer(enc, src_mask)\n",
    "\n",
    "        dec = self.pos_emb(self.token_emb(tgt) * math.sqrt(self.token_emb.embedding_dim))\n",
    "        for layer in self.dec_layers:\n",
    "            dec = layer(dec, enc, look_ahead_mask=tgt_mask, padding_mask=src_mask)\n",
    "\n",
    "        return self.fc_out(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AVieceQjhff-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SHxlVfsteRMa",
    "outputId": "56220021-f0f6-43a9-9b05-05ba6f88d37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 77.040996M\n",
      "Epoch [1/3]  Loss: 4.2592\n",
      "Epoch [2/3]  Loss: 3.1632\n",
      "Epoch [3/3]  Loss: 2.7719\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "num_epochs = 3\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6}M\")\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        src = batch['input_ids'].to(device)\n",
    "        tgt = batch['labels'].to(device)\n",
    "        # print(src,tgt,model)\n",
    "        # Teacher forcing: predict next tokens\n",
    "        outputs = model(src, tgt[:, :-1])  # (batch, seq_len-1, vocab_size)\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, tokenizer.vocab_size),\n",
    "            tgt[:, 1:].reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]  Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZlG4zN_jNrL",
    "outputId": "a6f6b346-7170-4f2b-a67c-1159bceeebe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  9 14:07:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.12              Driver Version: 550.90.12      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   53C    P0             90W /  400W |   35889MiB /  40960MiB |     42%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             66W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             57W /  400W |     579MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A    152084      C   /apps/codes/anaconda3/bin/python            35878MiB |\n",
      "|    3   N/A  N/A    142855      C   /apps/codes/anaconda3/bin/python              568MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
